{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'reliefF' from 'sklearn.feature_selection' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\NAFIZI~1.BRA\\AppData\\Local\\Temp/ipykernel_13212/88934344.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmutual_info_classif\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreliefF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'reliefF' from 'sklearn.feature_selection' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import reliefF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "# Load the dataset\n",
    "file_path = 'WA_Fn-UseC_-HR-Employee-Attrition.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "# Handle categorical variables if needed (encode labels)\n",
    "label_encoder = LabelEncoder()\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        df[column] = label_encoder.fit_transform(df[column])\n",
    "# Assume 'Attrition' is the target variable, and you want to predict it based on other features\n",
    "X = df.drop('Attrition', axis=1)\n",
    "y = df['Attrition']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate information gain for each feature\n",
    "info_gain_selector = SelectKBest(score_func=reliefF, k='all')\n",
    "info_gain_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get feature scores and indices\n",
    "feature_scores = info_gain_selector.scores_\n",
    "feature_indices = info_gain_selector.get_support(indices=True)\n",
    "\n",
    "# Create a DataFrame to display feature names and their scores\n",
    "feature_scores_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Information Gain Score': feature_scores\n",
    "})\n",
    "\n",
    "# Sort features by information gain (descending order)\n",
    "sorted_features_df = feature_scores_df.sort_values(by='Information Gain Score', ascending=False)\n",
    "\n",
    "# Display the top-k features with scores\n",
    "k = 30  # You can choose the number of top features\n",
    "selected_features_df = sorted_features_df.head(k)\n",
    "print(selected_features_df)\n",
    "\n",
    "# Select the top-k features based on information gain\n",
    "selected_features = sorted_features_df.head(k)['Feature'].tolist()\n",
    "\n",
    "# Filter the dataset to include only the selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Train a Random Forest classifier using the selected features\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy with Information Gain Feature Selection: {accuracy}\")\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print metrics and confusion matrix\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"MCC: {mcc}\")\n",
    "print(f\"AUC: {roc_auc}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Feature  Information Gain Score\n",
      "27         TotalWorkingYears                0.357522\n",
      "4           DistanceFromHome                0.347504\n",
      "0                        Age                0.339160\n",
      "30            YearsAtCompany                0.335072\n",
      "22         PercentSalaryHike                0.317540\n",
      "33      YearsWithCurrManager                0.302047\n",
      "31        YearsInCurrentRole                0.292292\n",
      "11                HourlyRate                0.291171\n",
      "19        NumCompaniesWorked                0.272384\n",
      "28     TrainingTimesLastYear                0.246651\n",
      "6             EducationField                0.246511\n",
      "14                   JobRole                0.238231\n",
      "9    EnvironmentSatisfaction                0.234667\n",
      "32   YearsSinceLastPromotion                0.230036\n",
      "5                  Education                0.226961\n",
      "13                  JobLevel                0.220388\n",
      "12            JobInvolvement                0.212297\n",
      "24  RelationshipSatisfaction                0.209362\n",
      "15           JobSatisfaction                0.202641\n",
      "29           WorkLifeBalance                0.187284\n",
      "26          StockOptionLevel                0.180008\n",
      "16             MaritalStatus                0.143673\n",
      "1             BusinessTravel                0.127812\n",
      "21                  OverTime                0.091054\n",
      "3                 Department                0.080333\n",
      "17             MonthlyIncome                0.080315\n",
      "23         PerformanceRating                0.038611\n",
      "18               MonthlyRate                0.030686\n",
      "10                    Gender                0.021261\n",
      "25             StandardHours                0.000540\n",
      "\n",
      "Model: Random Forest\n",
      "Random Forest Accuracy: 0.8707482993197279\n",
      "Precision: 0.5454545454545454\n",
      "Recall: 0.15384615384615385\n",
      "MCC: 0.23993262840419546\n",
      "\n",
      "Model: AdaBoost\n",
      "AdaBoost Accuracy: 0.8707482993197279\n",
      "Precision: 0.5121951219512195\n",
      "Recall: 0.5384615384615384\n",
      "MCC: 0.45043954978678896\n",
      "\n",
      "Model: CatBoost\n",
      "CatBoost Accuracy: 0.8707482993197279\n",
      "Precision: 0.5263157894736842\n",
      "Recall: 0.2564102564102564\n",
      "MCC: 0.3050560821104668\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "file_path = 'WA_Fn-UseC_-HR-Employee-Attrition.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        df[column] = label_encoder.fit_transform(df[column])\n",
    "\n",
    "X = df.drop('Attrition', axis=1)\n",
    "y = df['Attrition']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Min-Max scaling to features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE oversampling to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Calculate information gain for each feature\n",
    "info_gain_selector = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "info_gain_selector.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get feature scores and indices\n",
    "feature_scores = info_gain_selector.scores_\n",
    "feature_indices = info_gain_selector.get_support(indices=True)\n",
    "\n",
    "# Create a DataFrame to display feature names and their scores\n",
    "feature_scores_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Information Gain Score': feature_scores\n",
    "})\n",
    "\n",
    "# Sort features by information gain (descending order)\n",
    "sorted_features_df = feature_scores_df.sort_values(by='Information Gain Score', ascending=False)\n",
    "\n",
    "# Display the top-k features with scores\n",
    "k = 30  # You can choose the number of top features\n",
    "selected_features_df = sorted_features_df.head(k)\n",
    "print(selected_features_df)\n",
    "\n",
    "# Select the top-k features based on information gain\n",
    "selected_features = sorted_features_df.head(k)['Feature'].index\n",
    "\n",
    "# Filter the dataset to include only the selected features\n",
    "X_train_selected = X_train_resampled[:, selected_features]\n",
    "X_test_selected = X_test_scaled[:, selected_features]\n",
    "\n",
    "# Random Forest\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_rf_classifier = RandomForestClassifier(random_state=42, **best_params_rf)\n",
    "best_rf_classifier.fit(X_train_selected, y_train_resampled)\n",
    "y_pred_rf = best_rf_classifier.predict(X_test_selected)\n",
    "\n",
    "# AdaBoost\n",
    "adaboost_classifier = AdaBoostClassifier(random_state=42)\n",
    "param_grid_adaboost = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "grid_search_adaboost = GridSearchCV(adaboost_classifier, param_grid_adaboost, cv=5, scoring='accuracy')\n",
    "grid_search_adaboost.fit(X_train_selected, y_train_resampled)\n",
    "best_params_adaboost = grid_search_adaboost.best_params_\n",
    "best_adaboost_classifier = AdaBoostClassifier(random_state=42, **best_params_adaboost)\n",
    "best_adaboost_classifier.fit(X_train_selected, y_train_resampled)\n",
    "y_pred_adaboost = best_adaboost_classifier.predict(X_test_selected)\n",
    "\n",
    "# CatBoost\n",
    "catboost_classifier = CatBoostClassifier(random_state=42, silent=True)\n",
    "param_grid_catboost = {\n",
    "    'iterations': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'depth': [4, 6, 8]\n",
    "}\n",
    "grid_search_catboost = GridSearchCV(catboost_classifier, param_grid_catboost, cv=5, scoring='accuracy')\n",
    "grid_search_catboost.fit(X_train_selected, y_train_resampled)\n",
    "best_params_catboost = grid_search_catboost.best_params_\n",
    "best_catboost_classifier = CatBoostClassifier(random_state=42, **best_params_catboost, silent=True)\n",
    "best_catboost_classifier.fit(X_train_selected, y_train_resampled)\n",
    "y_pred_catboost = best_catboost_classifier.predict(X_test_selected)\n",
    "\n",
    "# Evaluate models\n",
    "models = {\n",
    "    'Random Forest': (best_rf_classifier, y_pred_rf),\n",
    "    'AdaBoost': (best_adaboost_classifier, y_pred_adaboost),\n",
    "    'CatBoost': (best_catboost_classifier, y_pred_catboost)\n",
    "}\n",
    "\n",
    "for model_name, (model, y_pred) in models.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "\n",
    "    # Calculate metrics for the model\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    # Confusion matrix for the model\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Print metrics and confusion matrix for the model\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"MCC: {mcc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Feature  Information Gain Score\n",
      "27         TotalWorkingYears                0.363907\n",
      "4           DistanceFromHome                0.351502\n",
      "30            YearsAtCompany                0.344413\n",
      "0                        Age                0.341836\n",
      "33      YearsWithCurrManager                0.317628\n",
      "22         PercentSalaryHike                0.313778\n",
      "11                HourlyRate                0.305346\n",
      "31        YearsInCurrentRole                0.286843\n",
      "19        NumCompaniesWorked                0.276335\n",
      "32   YearsSinceLastPromotion                0.248169\n",
      "5                  Education                0.241150\n",
      "6             EducationField                0.237545\n",
      "14                   JobRole                0.234822\n",
      "28     TrainingTimesLastYear                0.229660\n",
      "15           JobSatisfaction                0.225040\n",
      "29           WorkLifeBalance                0.219446\n",
      "24  RelationshipSatisfaction                0.204054\n",
      "9    EnvironmentSatisfaction                0.199358\n",
      "26          StockOptionLevel                0.198227\n",
      "13                  JobLevel                0.190436\n",
      "12            JobInvolvement                0.172781\n",
      "16             MaritalStatus                0.168964\n",
      "1             BusinessTravel                0.144267\n",
      "3                 Department                0.119126\n",
      "17             MonthlyIncome                0.082853\n",
      "21                  OverTime                0.066578\n",
      "2                  DailyRate                0.039168\n",
      "10                    Gender                0.035003\n",
      "23         PerformanceRating                0.011653\n",
      "18               MonthlyRate                0.011153\n",
      "\n",
      "Model: Random Forest\n",
      "Random Forest Accuracy: 0.9352226720647774\n",
      "Precision: 0.9608695652173913\n",
      "Recall: 0.9057377049180327\n",
      "MCC: 0.8717407225189047\n",
      "\n",
      "Model: AdaBoost\n",
      "AdaBoost Accuracy: 0.9190283400809717\n",
      "Precision: 0.9434782608695652\n",
      "Recall: 0.889344262295082\n",
      "MCC: 0.8392726773630755\n",
      "\n",
      "Model: CatBoost\n",
      "CatBoost Accuracy: 0.9412955465587044\n",
      "Precision: 0.9613733905579399\n",
      "Recall: 0.9180327868852459\n",
      "MCC: 0.883387871554498\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "file_path = 'WA_Fn-UseC_-HR-Employee-Attrition.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        df[column] = label_encoder.fit_transform(df[column])\n",
    "\n",
    "# Apply Min-Max scaling to features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(df.drop('Attrition', axis=1))\n",
    "y = df['Attrition']\n",
    "\n",
    "# Apply SMOTE oversampling to the entire dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "\n",
    "# Split the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate information gain for each feature\n",
    "info_gain_selector = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "info_gain_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get feature scores and indices\n",
    "feature_scores = info_gain_selector.scores_\n",
    "feature_indices = info_gain_selector.get_support(indices=True)\n",
    "\n",
    "# Create a DataFrame to display feature names and their scores\n",
    "feature_scores_df = pd.DataFrame({\n",
    "    'Feature': df.drop('Attrition', axis=1).columns,\n",
    "    'Information Gain Score': feature_scores\n",
    "})\n",
    "\n",
    "# Sort features by information gain (descending order)\n",
    "sorted_features_df = feature_scores_df.sort_values(by='Information Gain Score', ascending=False)\n",
    "\n",
    "# Display the top-k features with scores\n",
    "k = 30  # You can choose the number of top features\n",
    "selected_features_df = sorted_features_df.head(k)\n",
    "print(selected_features_df)\n",
    "\n",
    "# Select the top-k features based on information gain\n",
    "selected_features = sorted_features_df.head(k)['Feature'].index\n",
    "\n",
    "# Filter the dataset to include only the selected features\n",
    "X_train_selected = X_train[:, selected_features]\n",
    "X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "# Random Forest\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf.fit(X_train_selected, y_train)\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_rf_classifier = RandomForestClassifier(random_state=42, **best_params_rf)\n",
    "best_rf_classifier.fit(X_train_selected, y_train)\n",
    "y_pred_rf = best_rf_classifier.predict(X_test_selected)\n",
    "\n",
    "# AdaBoost\n",
    "adaboost_classifier = AdaBoostClassifier(random_state=42)\n",
    "param_grid_adaboost = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "grid_search_adaboost = GridSearchCV(adaboost_classifier, param_grid_adaboost, cv=5, scoring='accuracy')\n",
    "grid_search_adaboost.fit(X_train_selected, y_train)\n",
    "best_params_adaboost = grid_search_adaboost.best_params_\n",
    "best_adaboost_classifier = AdaBoostClassifier(random_state=42, **best_params_adaboost)\n",
    "best_adaboost_classifier.fit(X_train_selected, y_train)\n",
    "y_pred_adaboost = best_adaboost_classifier.predict(X_test_selected)\n",
    "\n",
    "# CatBoost\n",
    "catboost_classifier = CatBoostClassifier(random_state=42, silent=True)\n",
    "param_grid_catboost = {\n",
    "    'iterations': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'depth': [4, 6, 8]\n",
    "}\n",
    "grid_search_catboost = GridSearchCV(catboost_classifier, param_grid_catboost, cv=5, scoring='accuracy')\n",
    "grid_search_catboost.fit(X_train_selected, y_train)\n",
    "best_params_catboost = grid_search_catboost.best_params_\n",
    "best_catboost_classifier = CatBoostClassifier(random_state=42, **best_params_catboost, silent=True)\n",
    "best_catboost_classifier.fit(X_train_selected, y_train)\n",
    "y_pred_catboost = best_catboost_classifier.predict(X_test_selected)\n",
    "\n",
    "# Evaluate models\n",
    "models = {\n",
    "    'Random Forest': (best_rf_classifier, y_pred_rf),\n",
    "    'AdaBoost': (best_adaboost_classifier, y_pred_adaboost),\n",
    "    'CatBoost': (best_catboost_classifier, y_pred_catboost)\n",
    "}\n",
    "\n",
    "for model_name, (model, y_pred) in models.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "\n",
    "    # Calculate metrics for the model\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    # Confusion matrix for the model\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Print metrics and confusion matrix for the model\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"MCC: {mcc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Feature  Information Gain Score\n",
      "30            YearsAtCompany                0.300239\n",
      "0                        Age                0.291174\n",
      "27         TotalWorkingYears                0.286231\n",
      "4           DistanceFromHome                0.278307\n",
      "33      YearsWithCurrManager                0.276562\n",
      "31        YearsInCurrentRole                0.251302\n",
      "11                HourlyRate                0.239467\n",
      "14                   JobRole                0.235874\n",
      "22         PercentSalaryHike                0.234834\n",
      "19        NumCompaniesWorked                0.230882\n",
      "15           JobSatisfaction                0.197337\n",
      "28     TrainingTimesLastYear                0.193484\n",
      "9    EnvironmentSatisfaction                0.189542\n",
      "32   YearsSinceLastPromotion                0.184983\n",
      "13                  JobLevel                0.182612\n",
      "6             EducationField                0.180033\n",
      "26          StockOptionLevel                0.177734\n",
      "5                  Education                0.173893\n",
      "24  RelationshipSatisfaction                0.169677\n",
      "29           WorkLifeBalance                0.161796\n",
      "16             MaritalStatus                0.143704\n",
      "12            JobInvolvement                0.137778\n",
      "21                  OverTime                0.111546\n",
      "1             BusinessTravel                0.098441\n",
      "17             MonthlyIncome                0.064693\n",
      "3                 Department                0.058615\n",
      "2                  DailyRate                0.042704\n",
      "10                    Gender                0.026748\n",
      "20                    Over18                0.022495\n",
      "18               MonthlyRate                0.017811\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "file_path = 'WA_Fn-UseC_-HR-Employee-Attrition.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        df[column] = label_encoder.fit_transform(df[column])\n",
    "\n",
    "# Apply Min-Max scaling to features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(df.drop('Attrition', axis=1))\n",
    "y = df['Attrition']\n",
    "\n",
    "# Apply SMOTE-ENN oversampling to the entire dataset\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X_scaled, y)\n",
    "\n",
    "# Split the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate information gain for each feature\n",
    "info_gain_selector = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "info_gain_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get feature scores and indices\n",
    "feature_scores = info_gain_selector.scores_\n",
    "feature_indices = info_gain_selector.get_support(indices=True)\n",
    "\n",
    "# Create a DataFrame to display feature names and their scores\n",
    "feature_scores_df = pd.DataFrame({\n",
    "    'Feature': df.drop('Attrition', axis=1).columns,\n",
    "    'Information Gain Score': feature_scores\n",
    "})\n",
    "\n",
    "# Sort features by information gain (descending order)\n",
    "sorted_features_df = feature_scores_df.sort_values(by='Information Gain Score', ascending=False)\n",
    "\n",
    "# Display the top-k features with scores\n",
    "k = 30  # You can choose the number of top features\n",
    "selected_features_df = sorted_features_df.head(k)\n",
    "print(selected_features_df)\n",
    "\n",
    "# Select the top-k features based on information gain\n",
    "selected_features = sorted_features_df.head(k)['Feature'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Random Forest\n",
      "Random Forest Accuracy: 0.9728260869565217\n",
      "Precision: 0.9745762711864406\n",
      "Recall: 0.9829059829059829\n",
      "MCC: 0.9411965398254964\n",
      "\n",
      "Model: AdaBoost\n",
      "AdaBoost Accuracy: 0.9266304347826086\n",
      "Precision: 0.9224489795918367\n",
      "Recall: 0.9658119658119658\n",
      "MCC: 0.840548868965454\n",
      "\n",
      "Model: CatBoost\n",
      "CatBoost Accuracy: 0.9782608695652174\n",
      "Precision: 0.9747899159663865\n",
      "Recall: 0.9914529914529915\n",
      "MCC: 0.9530198366156643\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to include only the selected features\n",
    "X_train_selected = X_train[:, selected_features]\n",
    "X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "# Random Forest\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf.fit(X_train_selected, y_train)\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_rf_classifier = RandomForestClassifier(random_state=42, **best_params_rf)\n",
    "best_rf_classifier.fit(X_train_selected, y_train)\n",
    "y_pred_rf = best_rf_classifier.predict(X_test_selected)\n",
    "\n",
    "# AdaBoost\n",
    "adaboost_classifier = AdaBoostClassifier(random_state=42)\n",
    "param_grid_adaboost = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "grid_search_adaboost = GridSearchCV(adaboost_classifier, param_grid_adaboost, cv=5, scoring='accuracy')\n",
    "grid_search_adaboost.fit(X_train_selected, y_train)\n",
    "best_params_adaboost = grid_search_adaboost.best_params_\n",
    "best_adaboost_classifier = AdaBoostClassifier(random_state=42, **best_params_adaboost)\n",
    "best_adaboost_classifier.fit(X_train_selected, y_train)\n",
    "y_pred_adaboost = best_adaboost_classifier.predict(X_test_selected)\n",
    "\n",
    "# CatBoost\n",
    "catboost_classifier = CatBoostClassifier(random_state=42, silent=True)\n",
    "param_grid_catboost = {\n",
    "    'iterations': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'depth': [4, 6, 8]\n",
    "}\n",
    "grid_search_catboost = GridSearchCV(catboost_classifier, param_grid_catboost, cv=5, scoring='accuracy')\n",
    "grid_search_catboost.fit(X_train_selected, y_train)\n",
    "best_params_catboost = grid_search_catboost.best_params_\n",
    "best_catboost_classifier = CatBoostClassifier(random_state=42, **best_params_catboost, silent=True)\n",
    "best_catboost_classifier.fit(X_train_selected, y_train)\n",
    "y_pred_catboost = best_catboost_classifier.predict(X_test_selected)\n",
    "\n",
    "# Evaluate models\n",
    "models = {\n",
    "    'Random Forest': (best_rf_classifier, y_pred_rf),\n",
    "    'AdaBoost': (best_adaboost_classifier, y_pred_adaboost),\n",
    "    'CatBoost': (best_catboost_classifier, y_pred_catboost)\n",
    "}\n",
    "\n",
    "for model_name, (model, y_pred) in models.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "\n",
    "    # Calculate metrics for the model\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    # Confusion matrix for the model\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Print metrics and confusion matrix for the model\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"MCC: {mcc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
